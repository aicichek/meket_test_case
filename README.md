# meket_test_case
**1. Нарисовать схему модели данных, используя одну из известных методологий.**
![image](https://github.com/user-attachments/assets/de7b5b8d-0c4b-420c-8c49-59c2efb9c614)

**2. Объяснить, почему выбрана именно такая методология.**
Методология звездной схемы была выбрана по следующим причинам:
  Простота: Звездная схема проста в реализации и понимании. Она хорошо подходит для OLAP (онлайн аналитическая обработка) запросов.
  Производительность: Звездная схема позволяет эффективно выполнять запросы благодаря денормализации данных, что уменьшает количество необходимых объединений таблиц.
  Масштабируемость: Звездная схема легко масштабируется и позволяет добавлять новые таблицы измерений или фактов без значительных изменений в существующей схеме.
  
**3. На основе схемы из п.1 реализовать ELT/ETL процесс, который по расписанию инкрементально выгружает данные из БД-источника в DWH.**
Требуемый DAG лежит в папке dags etl_dwh_process.py

**4. Написать SQL код витрины данных с уроками для анализа и визуализации**
Скрипт лежит в папке sql  data_mart1.sql

**Что сделано:**
  В docker compose развернут airflow и 2 БД.
    1 из БД автоматически при запуске заполняется таблицами и тестовыми данными.
    2 БД нужна для создание в ней хранилища и переноса в него данных.
  В папке Scripts лежат скрипты, спользуемые ДАГами в процессе работы.
  В папке SQL лежит скрипт для создания витрины.
  В Папке dags лежат ДАГи:
    create_dwh_schema.py создает в конечной бд новую схему и создает в ней таблицы.
    etl_dwh_process.py выполняет перенос данных из БД-источника в хранилище.

  **Как запустить:** 
  1. В терминале прописываем sudo docker compose up
  2. Ждем запуска и переходим в airflow http://localhost:8080/
  3. Авторизуемся: admin:admin
  4. Переходим на вкладку admin/connections
  5. Создем 2 подключчения:
      Connection Id:source_db Connection Type: postgres Host:postgres_source Schema:source_db Login:airflow Password:airflow Port:5432
      Connection Id:target_db Connection Type: postgres Host:postgres_target Schema:target_db Login:airflow Password:airflow Port:5432
  6. Запускаем DAG create_dwh_schema
  7. Запускаем DAG etl_dwh_process
     В результате получаем созданное хранилище, наполненное перенесенными из источника данными
